강화 학습 
: 지도학습 처럼 정답이 있지도 않고 비지도 학습 처럼 데이터 만을 기반으로 학습하는 방법이 아니다 

정의
에이전트 라는 존재가 환경과 상호 작용하며,이환경에는 보상이라는 기준이있어서
다양한 시행착오를 거쳐 보상을 최대로 만드는게 목적이다.

비교적 명확한 보상을 설정할수 있는 문제를 해결하는데 사용되고 있다.
강화학습은 빠르게 발전해오면서 지금껏 인공지능으로 해결하기 힘들다고 생각 한 많은 문제들을 해결 하였다.

보상을 최대화 하는 의사 결정 전략 
- 즉 순차적인 행동들을 알아나가는 방법


순차적인 행동을 알아가는 알고리즘이 
Markov Decision Process

MDP 는 행동 , 상태, 보상함수 ,상태변환 확률 , 감가율이 라는 구성요소가 있다.


에이전트  - 의사 결정을 하는 역할을 맡고 있다.

환경  - 에이전트 의 의사 결정을 반영하여 에이전트에세 반영된 정보를 주는역할을 담당합니다

상태 - 에이전트는 상태를 기반으로 의사결정을 진행한다.
	의사결정을 하기위해서 사용되는 관측값 행동 보상 을 가공한 정보이다

행동  - 에이전트가 의사 결정을 통해 취할수있는 행동  이를 At 라고 표기한다.
           이산적 행동: 에이전트에게 주어지는 행동의 선택지가 있으며   에이전트는 그중 하나를 선택한다
 
           연속적행동 : 선택지마다 특정값을 수치로 입력하게 되고 에이전트는 입력된 값만 큼 행동하게 된다.

관측 - 환경 에서 제공해주는 정보이다 
1.시각적 관측 : 현재 상태의 정보를 이미지로 표현한 것이며 
2.수치적 관측 : 이미지가 아닌 수치로만 표현한 관측을 의미한다

보상함수 ****
에이전트가 특정 상태에서 특정 행동을 취했을때 보상을 받게되고 에이전트는 이 보상정보를 통해 학습을
진행한다.

현재 상태에서 특정 행동을 취했을떄 얻는 기대값

Ras=E[Rt+1|St=s,At=a]
St : 현재 상태 
At : 취할수 있는 행동
Rt+1 : St 와 At 를 통해 얻을수 있는 보상을 의미한다

강화학습 에서는 에피소드가 끝났을떄 에이전트가 지나왔던 행동에 대한 정보를 기록하게 됩니다.
그다음 에피소드에 대한 의사 결정을 하게된다.
에피소드가 끝나면 이 에피소드를 통해 얻게 된 정보를 기록을 업데이트 하며 이러한 과정을 반복하게 된다.


그러면 에이전트는 어떠한 정보를 에피소드에 기록했을때 더 좋은 의사 결정을 하게 될것인가?

더 나은 의사 결정을 하기 위해 현재 스탭에서 받았던 보상으로 부터 에피소드가 끝날때까지 받았던 보상들을 더한 것을 정보로 이용한다.


에이전트가 여러가지 상황중에서 어떤 정보가 더 좋은지 모를떄 사용하는 제도가 감가율이다.

감가율 이라는 개념은 그리스 문자중 3번쨰 γ 0 ~1 사이의 값으로 설정 1에 가까울수록 미래의 보상의 더많은 가중치를 두게된다

감가율이 반영된 보상정보는 

Gt = R(t+1) +γR(t+2) + .....  + γ^T-t R(t+1) 
 
현재 스탭으로 부터 받았던 보상부터 에피소드가 끝날때까지 받았던 보상들의 감가율을 스탭 차이 만큼 곱해서 더해주면 된다.


하지만 에이전트가 꼭 자신의 아는길만 간다는 보장이 없으므로
강화 학습에서는 에이전트에게 무작위 움직임이라는 개념이 도입하는 탐험이 라는 시스템도 있다.

탐험과 대립되는 개념은 이용이라는 개념으로 에이전트가 찾아놓은 길로하여 계속해서 선택하고 움직이게 되는것을 의미한다.

