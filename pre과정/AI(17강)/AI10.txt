 인공지능 은 학습 과정을 알수 없는 블랙 박스 였다.


결정 트리 
설명이 중요할때 아주 유용하게 사용된다.
일련의 질문을 통해 데이터를 분해하는 모델이다.
범주형 변수(오늘 할일이 있는가?), 실수형 변수(와인의 도수가 0.5 이상이다?)에도 둘다 적용이 된다.

결정 트리는 트리의 루트 부터 시작해 정보 이득이라는 값이 최대가 되는 특성으로 데이터를 나누게 된다.

마지막 노드 리프노드 가 순수해 질때 까지 모든 자식 노드에서 분할작업을 수행

그러나 이때 너무 분할을 자주 하게 되면 트리가 너무 깊어져서 이는 곧 과적합을 불러 일으킬수 있다.

일반적으로 트리 최대 깊이를 제안 하여 트리 가지치기를 제안한다 
트리 가지치기  = pruning

목적함수의 목적은 
가장 정보가 풍부한 특성으로 노드를 나누기 위함 이다.
트리 알고리즘으로 최적화 해줘야 한다

-> 정보 이득을 최대화 해야 한다 

결정 트리 의 정보 이득 공식 
IG(Dp , F) = I(Dp) - ∑ Nj/Np * I(Dj)
F = 어떠한 분할 기법을 사용 할것 인가
Dp,Dj = 부모와 j 번쨰 자식노드의 데이터 셋 (즉 부모노드의 수 = 자식노드들의 수의 합)
I = 불순도 지표 레이블이 얼마나 순수하게 분포되어 있는가 알아보는 지표이다.
Np,Nj = 부모 자식 노드에 있는 전체 데이터의 개수 이다

즉 정보 이득은 부모노드의 불순도  -  자식노드의 불순도 합 
+ 자식노드의 불순도가 낮을수록 정보 이득은 커지게 된다.

대부분의 라이브러리 들은 구현을 간단하게 하고 탐색 공간을 줄이기 위해 이진 탐색 트리를 사용하게 된다.
+ 이진 탐색 트리로 하면 부모노드를 두개 의 자식노드로 나눌수 있다는 내용입니다.
수식 재정의 
IG(Dp , F) = I(Dp) -  Nleft/Np * I(Dleft) - Nright/Np * I(Dright)

이진 분류를 하기 위해서는 어떠한 분할 조건이랑 불순도 지표를 사용해야되는지 
알아야 한다

불순도 종류는 
1.지니 불순도
공식 : 
Ig(t) = ∑ p(i|t) (1- p(i|t))  = 1 - ∑p(i|t) ^2


클래스 가 완벽하게 섞여 있을떄 최대 이다 

가지치기 수준을 바꿔 가면서 데이터 튜닝 하는것이 좋다. 

지니불순도가 엔트로피와 분류오차의 중간값이다.


2.엔트로피
Ih(t) = - ∑ p(i|t) log2p(i|t)
p(i|t) 는 특정 노드 t 에서 i에 속한 데이터 비율을 의미한다.

한노드의 모든 데이터 가 같은 클래스 라면 엔트로피 수치는 0
클래스 분포가 균등 하다면 엔트로피는 최대 1
쉽게 설명하면 아이스크림 을 사는데 모든 아이스크림이 쌍쌍바 라면 0 이고 
쌍쌍바 누가봐 메로나 균등하게 분포가 잘되있다면 최대 1 이라고 말할수 있다.

엔트로피는 트리의 상호 의존 정보를 최대화 하는것이 목적이다.



3.분류 오차 등이 있다.
분류 오차 역시 두 클래스가 같은 비율일때 최대값 0.5를 출력한다.
IE = 1 - max{p(i|t)} 

노드의 클래스 확률 변화에 많이 둔감한 편이다.


최근접 이웃
올바른 매개변수를 찾기 위한 학습 과정을 찾는다
훈련과정을 지나지 않고 결과를 도출하는 머신러닝 알고리즘이다


알고리즘 실행 마다  모든 학습 데이터를 통해 분류를 진행 
매번 실행 할때 마다 학습 데이터 필요

계산 복잡도 또한 훈련 데이터의 개수에 비례하여 

K 와 측정 기준을 선택 
숫자 K 와 거리 측정 기준을 선택 해야 한다.
분류하려는 미지의 데이터에서 K 개의 최근접 이웃을 찾는다
다수결 투표를 진행 투표를 통해 새로운 데이터를 예측한다

차원의 저주 : 고정된 크기에 훈련 데이터 셋 차원이 
늘어남에 따라 특성 공간이 점점 희소해지는 현상 

방지 하려면 올바른 변수 선택 차원 축소 기법 선택이 중요하다.
